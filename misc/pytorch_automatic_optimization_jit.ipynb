{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automatic optimization with the PyTorch JIT\n",
    "## a worked example\n",
    "*by Thomas Viehmann <tv@lernapparat.de>*\n",
    "\n",
    "Today, I would like to discuss in detail some aspects of optimizing code in models, and in particular how you can let the PyTorch JIT optimize things for you.\n",
    "\n",
    "We will use the *Intersection over Union* loss commonly used in training detection models as an example and explore various ways to implement it in PyTorch.\n",
    "\n",
    "The intersection over union (or IoU) loss arises in training detection networks.\n",
    "Given two axis-parallel rectangles (blue and red), we wish to compute the quotient between the are in the intersection (which is a rectangle again) and the union. In colors:\n",
    "\n",
    "![iou.svg](iou.svg)\n",
    "\n",
    "As the intersection is always contained in the union, we know that $0 \\leq IoU \\leq 1$ (with the optimum being $1$, so strictly speaking $-IoU$ would be a loss).\n",
    "\n",
    "Note that if we have the area of the intersection and of the two rectangles, we can also express the area of the union as the sum areas of the two rectangles minus the area of the intersection (which is contained twice in the sum).\n",
    "\n",
    "Let $(x_1, y_1, w_1, h_1$) be the coordinates top left and the width and the height of the first rectangle and $(x_2, y_2, w_2, h_2$) those of the second.\n",
    "\n",
    "The intersection is easily calculated: If we have the top left and bottom right coordinates (and our coordinate system has increasing $y$ from top to bottom), we can take the maximum of the top left coordinates and the minimum of the bottom right coordinates.\n",
    "So we have[^1]\n",
    "$$\n",
    "x_I = \\max(x_1, x_2), \\qquad y_I = \\max(y_1, y_2)\n",
    "$$\n",
    "and - we need to calculate the bottom right corners, take the minimum and transform back to width and hight -\n",
    "$$\n",
    "w_I = \\min(x_1 + w_1, x_2 + w_2)-x_I.\n",
    "$$\n",
    "But there is a slight complication when the rectangles don't intersect: then our formulae do not work but instead give us the rectangle \"between\" the two but with the corner points exchanged. This means that then $w_i$ calculated as above is actually negative, so we can fix this by enforcing a minimum of $0$\n",
    "$$\n",
    "w_I = \\max \\left( \\min(x_1 + w_1, x_2 + w_2)-x_I,0\\right), \\qquad h_I = \\max \\left( \\min(y_1 + h_1, y_2 + h_2)-y_I,0\\right).\n",
    "$$\n",
    "Note that these last maxmimizations with a constant would be performed in PyTorch using the `torch.clamp` function, while the (elementwise) maximum and minimum between two tensors is computed using `torch.min` and `torch.max`.\n",
    "\n",
    "Speaking of PyTorch, enough of the theory, let's move to practical things!\n",
    "\n",
    "\n",
    "[^1]: I use $I$ here to mean *Intersection*, it's not an index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.utils.cpp_extension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The formulas above readily translate into a PyTorch function. Just to be safe, we clamp the the denominator to be at least $10^{-5}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ratio_iou(x1, y1, w1, h1, x2, y2, w2, h2, eps=1e-5):\n",
    "    xi = torch.max(x1, x2)                                 # Intersection\n",
    "    yi = torch.max(y1, y2)\n",
    "    wi = torch.clamp(torch.min(x1+w1, x2+w2) - xi, min=0)\n",
    "    hi = torch.clamp(torch.min(y1+h1, y2+h2) - yi, min=0)\n",
    "    area_i = wi * hi                                       # Area Intersection\n",
    "    area_u = w1 * h1 + w2 * h2 - wi * hi                   # Area Union\n",
    "    return area_i / torch.clamp(area_u, min=eps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function will is vector-ready just by passing in a multi-dimensional tensor.\n",
    "Let's try it out with some dummy data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0233],\n",
       "        [0.0000, 0.0000, 0.0000,  ..., 0.0444, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "        ...,\n",
       "        [0.0614, 0.0000, 0.0000,  ..., 0.1205, 0.0000, 0.0599],\n",
       "        [0.0000, 0.0000, 0.0000,  ..., 0.2437, 0.0000, 0.0110],\n",
       "        [0.0000, 0.5228, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1, y1, w1, h1, x2, y2, w2, h2 = torch.randn(8, 100, 1000, device='cuda').exp()\n",
    "ratio_iou(x1, y1, w1, h1, x2, y2, w2, h2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without looking too much at the results, it seems to work.\n",
    "\n",
    "Let us take a short digression here. As you may know, PyTorch provides functional interfaces \n",
    "in `torch.nn.functional` (often also known as `F`) as well as modules (in `torch.nn`, commonly imported as `nn`)[^2]. It does so for typical neural network components as well as the loss functions. We might wonder which is preferable for our own modelling. It is, in the end, a question of style, but I would suggest the following as a good rule of thumb: If it has (significant) parameters or even state, use the module interface (so subclass `nn.Module`). If it has not, define a function as the above. I also do this when using PyTorch's functions - e.g. I usually spell out my forward and prefer to use the function `F.relu` over the module `nn.Relu`.\n",
    "\n",
    "[^2]: I might say that I usually just type out the modules instead for importing them under short names.\n",
    "\n",
    "But enough of the digression. Can our ratio_iou calculation be made more efficient?\n",
    "\n",
    "One common thought when trying to make Python things more efficient is moving to C++. Fortunately PyTorch makes it very straightforward to do so, by the way of C++ extensions or custom operators. Both work the same except for the actual bindings. The difference between them is that functions in PyTorch extensions can take any parameters (by using the library PyBind11) while custom operators are restricted to the types that PyTorch supports (e.g. Tensors, `int64_t`, `double`, `std::string`, `IntList` and `TensorList`). The advantage of custom operators is that they can be used with the JIT and in C++, too.\n",
    "\n",
    "Happily, we can just type our C-Code into a cell and have PyTorch  compile it for us. Let's do a custom operator that follows exactly the Python function above:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using /tmp/torch_extensions as PyTorch extensions root...\n",
      "Emitting ninja build file /tmp/torch_extensions/libsuperiou/build.ninja...\n",
      "Building extension module libsuperiou...\n",
      "Loading extension module libsuperiou...\n"
     ]
    }
   ],
   "source": [
    "csrc = \"\"\"\n",
    "#include <torch/script.h>\n",
    "\n",
    "using namespace torch;\n",
    "\n",
    "Tensor iou_native(const Tensor& x1, const Tensor& y1, const Tensor& w1, const Tensor& h1,\n",
    "                  const Tensor& x2, const Tensor& y2, const Tensor& w2, const Tensor& h2) {\n",
    "\n",
    "    auto xi = torch::max(x1, x2);\n",
    "    auto yi = torch::max(y1, y2);\n",
    "    auto wi = torch::clamp(torch::min(x1+w1, x2+w2) - xi, 0);\n",
    "    auto hi = torch::clamp(torch::min(y1+h1, y2+h2) - yi, 0);\n",
    "    auto area_i = wi * hi;\n",
    "    auto area_u = w1 * h1 + w2 * h2 - wi * hi;\n",
    "    return area_i / torch::clamp(area_u, 1e-5);\n",
    "}\n",
    "\n",
    "\n",
    "static auto registry =\n",
    "  torch::jit::RegisterOperators(\"super_iou::iou_native\", &iou_native);\n",
    "\"\"\"\n",
    "\n",
    "torch.utils.cpp_extension.load_inline(\"libsuperiou\", csrc, is_python_module=False, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That was easy enough! Now we have a custom op unter `torch.ops`, the name it is available under is determined by the string argument to RegisterOperators - here `torch.ops.super_iou.iou_native`. (Note: If you get an error about \"multiple overloads\", you'll have to reload your kernel and start again... While PyTorch extensions support re-building and re-loading, custom operators run into trouble with that.) Let's see if it gives the same result as the Python version:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(ratio_iou(x1, y1, w1, h1, x2, y2, w2, h2)==torch.ops.super_iou.iou_native(x1, y1, w1, h1, x2, y2, w2, h2)).all().item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It works. Note that in general it is safer to use `torch.almost_equal` or print `(a-b).abs().max()` to deal with numerical precision. But here, `==` works well, too.\n",
    "\n",
    "So how about timings? Note that we need to call `torch.cuda.synchronize()` to get valid timings on the GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 loops, best of 3: 1.07 ms per loop\n",
      "1000 loops, best of 3: 987 µs per loop\n"
     ]
    }
   ],
   "source": [
    "def taketime(fn):\n",
    "    _ = fn(x1, y1, w1, h1, x2, y2, w2, h2)\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "torch.cuda.synchronize()\n",
    "%timeit taketime(ratio_iou)\n",
    "%timeit taketime(torch.ops.super_iou.iou_native)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that there is difference of about 5% n cuda, if we did this with CPU tensors, there would be no significant difference. Depending on the nature of the calculation, this is a typical result. For the `lltm` model in the PyTorch C++-Extension tutorial, you get a speedup of about 10% by moving to C++. But this involves a loop over the input sequence, so calls quite a few tensor operation. Here we only have a handful of operations, so moving to C++ offers little performance gain by itself.\n",
    "\n",
    "What is relatively slow about our code is that each operation stores intermediate results in tensors and the next operation reads those to continue. If we write our own kernel, that can be helped. I consider this the \"classic way\" of optimizing models.\n",
    "The `TensorAccessor` (for CPU) / `PackedTensorAccessor` (for transferring sizes and strides to GPU) classes provide a convenient interface for element access. As you would in production, we multiplex the floating types through templates in `scalar_t`.\n",
    "For simplicity, we only deal with 1-d tensors (this is the second argument to anything `accessor`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using /tmp/torch_extensions as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /tmp/torch_extensions/iou_op/build.ninja...\n",
      "Building extension module iou_op...\n",
      "Loading extension module iou_op...\n"
     ]
    }
   ],
   "source": [
    "csrc = \"\"\"\n",
    "#include <torch/script.h>\n",
    "#include <ATen/Parallel.h>\n",
    "\n",
    "using namespace torch;\n",
    "\n",
    "// The cuda kernel is easy enough\n",
    "template<typename scalar_t>\n",
    "__global__ void iou_kernel_gpu(PackedTensorAccessor<scalar_t, 1> result,\n",
    "                          PackedTensorAccessor<scalar_t, 1> x1,\n",
    "                          PackedTensorAccessor<scalar_t, 1> y1,\n",
    "                          PackedTensorAccessor<scalar_t, 1> w1,\n",
    "                          PackedTensorAccessor<scalar_t, 1> h1,\n",
    "                          PackedTensorAccessor<scalar_t, 1> x2,\n",
    "                          PackedTensorAccessor<scalar_t, 1> y2,\n",
    "                          PackedTensorAccessor<scalar_t, 1> w2,\n",
    "                          PackedTensorAccessor<scalar_t, 1> h2\n",
    "                          ) {\n",
    "    int i = threadIdx.x + blockDim.x * blockIdx.x;\n",
    "    if (i >= x1.size(0)) // we might have more threads than work to do in the last block\n",
    "      return;\n",
    "    // This should look very familiar. We could try reading each element only once, but let's keep it simple.\n",
    "    scalar_t xi = max(x1[i], x2[i]);\n",
    "    scalar_t yi = max(y1[i], y2[i]);\n",
    "    scalar_t wi = max(min(x1[i]+w1[i], x2[i]+w2[i]) - xi, static_cast<scalar_t>(0));\n",
    "    scalar_t hi = max(min(y1[i]+h1[i], y2[i]+h2[i]) - yi, static_cast<scalar_t>(0));\n",
    "    scalar_t area_i = wi * hi;\n",
    "    scalar_t area_u = w1[i] * h1[i] + w2[i] * h2[i] - area_i;\n",
    "    result[i] = area_i / max(area_u, static_cast<scalar_t>(0.00001f));\n",
    "}\n",
    "\n",
    "// The CPU kernel is looks similar, we could also just put it in the main function...\n",
    "template<typename scalar_t>\n",
    "void iou_kernel_cpu(TensorAccessor<scalar_t, 1> result,\n",
    "                    TensorAccessor<scalar_t, 1> x1,\n",
    "                    TensorAccessor<scalar_t, 1> y1,\n",
    "                    TensorAccessor<scalar_t, 1> w1,\n",
    "                    TensorAccessor<scalar_t, 1> h1,\n",
    "                    TensorAccessor<scalar_t, 1> x2,\n",
    "                    TensorAccessor<scalar_t, 1> y2,\n",
    "                    TensorAccessor<scalar_t, 1> w2,\n",
    "                    TensorAccessor<scalar_t, 1> h2) {\n",
    "\n",
    "    // we use CPU parallelization\n",
    "    constexpr int64_t GRAIN_SIZE = 8192; // minimum grain size for parallel execution\n",
    "    at::parallel_for(0, x1.size(0), GRAIN_SIZE, [&](int64_t i_begin, int64_t i_end) {\n",
    "        for (int64_t i = i_begin; i < i_end; ++i) {\n",
    "            scalar_t xi = max(x1[i], x2[i]);\n",
    "            scalar_t yi = max(y1[i], y2[i]);\n",
    "            scalar_t wi = max(min(x1[i]+w1[i], x2[i]+w2[i]) - xi, static_cast<scalar_t>(0));\n",
    "            scalar_t hi = max(min(y1[i]+h1[i], y2[i]+h2[i]) - yi, static_cast<scalar_t>(0));\n",
    "            scalar_t area_i = wi * hi;\n",
    "            scalar_t area_u = w1[i] * h1[i] + w2[i] * h2[i] - area_i;\n",
    "            result[i] = area_i / max(area_u, static_cast<scalar_t>(0.00001f));\n",
    "        }\n",
    "    });\n",
    "}\n",
    "\n",
    "\n",
    "torch::Tensor iou_forward(const Tensor& x1, const Tensor& y1, const Tensor& w1, const Tensor& h1,\n",
    "                          const Tensor& x2, const Tensor& y2, const Tensor& w2, const Tensor& h2) {\n",
    "  auto res = torch::empty_like(x1);\n",
    "  for (auto& t : {x1, y1, w1, h1, x2, y2, w2, h2}) {\n",
    "     AT_ASSERTM(t.dim()==1 && t.size(0)==x1.size(0) && t.device()==x1.device() && t.dtype()==x1.dtype(),\n",
    "                \"tensors are not of same shape and kind\");\n",
    "  }\n",
    "  if (x1.is_cuda()) {\n",
    "    dim3 block(512);\n",
    "    dim3 grid((x1.size(0)+511)/512);\n",
    "    AT_DISPATCH_FLOATING_TYPES(x1.type(), \"iou\", [&] {\n",
    "      iou_kernel_gpu<scalar_t><<<grid,block>>>(res.packed_accessor<scalar_t, 1>(),\n",
    "                              x1.packed_accessor<scalar_t, 1>(),\n",
    "                              y1.packed_accessor<scalar_t, 1>(),\n",
    "                              w1.packed_accessor<scalar_t, 1>(),\n",
    "                              h1.packed_accessor<scalar_t, 1>(),\n",
    "                              x2.packed_accessor<scalar_t, 1>(),\n",
    "                              y2.packed_accessor<scalar_t, 1>(),\n",
    "                              w2.packed_accessor<scalar_t, 1>(),\n",
    "                              h2.packed_accessor<scalar_t, 1>());\n",
    "    });\n",
    "  } else {\n",
    "    AT_DISPATCH_FLOATING_TYPES(x1.type(), \"iou\", [&] {\n",
    "      iou_kernel_cpu<scalar_t>(res.accessor<scalar_t, 1>(),\n",
    "                              x1.accessor<scalar_t, 1>(),\n",
    "                              y1.accessor<scalar_t, 1>(),\n",
    "                              w1.accessor<scalar_t, 1>(),\n",
    "                              h1.accessor<scalar_t, 1>(),\n",
    "                              x2.accessor<scalar_t, 1>(),\n",
    "                              y2.accessor<scalar_t, 1>(),\n",
    "                              w2.accessor<scalar_t, 1>(),\n",
    "                              h2.accessor<scalar_t, 1>());\n",
    "    });  \n",
    "  }\n",
    "  return res;\n",
    "}\n",
    "\n",
    "torch::Tensor iou_native(const Tensor& x1, const Tensor& y1, const Tensor& w1, const Tensor& h1,\n",
    "                         const Tensor& x2, const Tensor& y2, const Tensor& w2, const Tensor& h2) {\n",
    "\n",
    "    auto xi = torch::max(x1, x2);\n",
    "    auto yi = torch::max(y1, y2);\n",
    "    auto wi = torch::clamp(torch::min(x1+w1, x2+w2) - xi, 0);\n",
    "    auto hi = torch::clamp(torch::min(y1+h1, y2+h2) - yi, 0);\n",
    "    auto area_i = wi * hi;\n",
    "    auto area_u = w1 * h1 + w2 * h2 - wi * hi;\n",
    "    return area_i / torch::clamp(area_u, 1e-5);\n",
    "}\n",
    "\n",
    "\n",
    "static auto registry =\n",
    "  torch::jit::RegisterOperators(\"super_iou2::iou_forward\", &iou_forward)\n",
    "    .op(\"super_iou2::iou_native\", &iou_native);\n",
    ";\n",
    "\"\"\"\n",
    "\n",
    "torch.utils.cpp_extension.load_inline(\"iou_op\", \"\", csrc, is_python_module=False, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Phew. That was a bit tedious, but let's see if it works!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check gpu 1\n",
      "check cpu 1\n"
     ]
    }
   ],
   "source": [
    "x1, y1, w1, h1, x2, y2, w2, h2 = [t.view(-1) for t in [x1, y1, w1, h1, x2, y2, w2, h2]]\n",
    "\n",
    "print (\"check gpu\", (ratio_iou(x1, y1, w1, h1, x2, y2, w2, h2)==torch.ops.super_iou.iou_native(x1, y1, w1, h1, x2, y2, w2, h2)).all().item())\n",
    "print (\"check cpu\", (torch.ops.super_iou2.iou_forward(x1.cpu(), y1.cpu(), w1.cpu(), h1.cpu(), x2.cpu(), y2.cpu(), w2.cpu(), h2.cpu())\n",
    "       == torch.ops.super_iou2.iou_forward(x1.cpu(), y1.cpu(), w1.cpu(), h1.cpu(), x2.cpu(), y2.cpu(), w2.cpu(), h2.cpu())).all().item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So it seems to work, let's time things again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The slowest run took 8.34 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
      "10000 loops, best of 3: 81.6 µs per loop\n",
      "1000 loops, best of 3: 1.05 ms per loop\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.synchronize()\n",
    "%timeit taketime(torch.ops.super_iou2.iou_forward)\n",
    "%timeit taketime(ratio_iou)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that is a lot faster!\n",
    "\n",
    "However, it is not usable as is: We do not have a backward. So we need two more kernels?\n",
    "Can we get something that is fast and doesn't need us to write all the infrastructure?\n",
    "\n",
    "It turns out we can. The PyTorch JIT has two awesome components, the *fuser* and the *autodiff* that will automatically create kernels for us. (There is a limitation, here, we need to specify the `max` argument to clamp in order for this here to work.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "@torch.jit.script\n",
    "def ratio_iou_scripted(x1, y1, w1, h1, x2, y2, w2, h2):\n",
    "    xi = torch.max(x1, x2)                                    # Intersection (yi similarly)\n",
    "    yi = torch.max(y1, y2)                                    # Intersection (yi similarly)\n",
    "    wi = torch.clamp(torch.min(x1+w1, x2+w2) - xi, min=0, max=math.inf)\n",
    "    hi = torch.clamp(torch.min(y1+h1, y2+h2) - yi, min=0, max=math.inf)\n",
    "    area_i = wi * hi                                      # Area Intersection\n",
    "    area_u = w1 * h1 + w2 * h2 - wi * hi    # Area Union\n",
    "    return area_i / torch.clamp(area_u, min=1e-5, max=math.inf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check 1.7881393432617188e-07\n"
     ]
    }
   ],
   "source": [
    "print(\"check\", (ratio_iou_scripted(x1, y1, w1, h1, x2, y2, w2, h2)-ratio_iou(x1, y1, w1, h1, x2, y2, w2, h2)).abs().max().item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's time it again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000 loops, best of 3: 81.5 µs per loop\n",
      "10000 loops, best of 3: 157 µs per loop\n",
      "1000 loops, best of 3: 1.05 ms per loop\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.synchronize()\n",
    "%timeit taketime(torch.ops.super_iou2.iou_forward)\n",
    "%timeit taketime(ratio_iou_scripted)\n",
    "%timeit taketime(ratio_iou)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not bad! We got a more than 6x speedup just by putting @torch.jit.script above our function.\n",
    "While apparent factor of two off the hand-crafted kernel still isn't ideal, part of that is that the size of the tensors isn't that large. Going to 10 Million elements, we are only 25% slower than the handwritten kernel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 loops, best of 3: 1.02 ms per loop\n",
      "1000 loops, best of 3: 1.28 ms per loop\n"
     ]
    }
   ],
   "source": [
    "x1, y1, w1, h1, x2, y2, w2, h2 = torch.randn(8, 10_000_000, device='cuda').exp()\n",
    "torch.cuda.synchronize()\n",
    "%timeit taketime(torch.ops.super_iou2.iou_forward)\n",
    "%timeit taketime(ratio_iou_scripted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How did that work? We can look at the graph the JIT has built for our calculation: You see that the main graph defers to a `FusionGroup`. The fusion group represents the graph that will be compiled into our custom kernel. (Note: I assume here that you run this with parameters *not* requiring gradients, we'll repeat the same with gradients below.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "graph(%x1 : Float(*)\n",
       "      %y1 : Float(*)\n",
       "      %w1 : Float(*)\n",
       "      %h1 : Float(*)\n",
       "      %x2 : Float(*)\n",
       "      %y2 : Float(*)\n",
       "      %w2 : Float(*)\n",
       "      %h2 : Float(*)) {\n",
       "  %32 : Float(*) = prim::FusionGroup_0(%w2, %h2, %w1, %h1, %y2, %y1, %x2, %x1)\n",
       "  return (%32);\n",
       "}\n",
       "with prim::FusionGroup_0 = graph(%14 : Float(*)\n",
       "      %15 : Float(*)\n",
       "      %17 : Float(*)\n",
       "      %18 : Float(*)\n",
       "      %34 : Float(*)\n",
       "      %37 : Float(*)\n",
       "      %51 : Float(*)\n",
       "      %54 : Float(*)) {\n",
       "  %xi : Float(*) = aten::max(%54, %51)\n",
       "  %yi : Float(*) = aten::max(%37, %34)\n",
       "  %55 : int = prim::Constant[value=1]()\n",
       "  %56 : Float(*) = aten::add(%54, %17, %55)\n",
       "  %52 : int = prim::Constant[value=1]()\n",
       "  %53 : Float(*) = aten::add(%51, %14, %52)\n",
       "  %50 : Float(*) = aten::min(%56, %53)\n",
       "  %46 : int = prim::Constant[value=1]()\n",
       "  %47 : Float(*) = aten::sub(%50, %xi, %46)\n",
       "  %41 : int = prim::Constant[value=0]()\n",
       "  %42 : float = prim::Constant[value=inf]()\n",
       "  %wi : Float(*) = aten::clamp(%47, %41, %42)\n",
       "  %38 : int = prim::Constant[value=1]()\n",
       "  %39 : Float(*) = aten::add(%37, %18, %38)\n",
       "  %35 : int = prim::Constant[value=1]()\n",
       "  %36 : Float(*) = aten::add(%34, %15, %35)\n",
       "  %33 : Float(*) = aten::min(%39, %36)\n",
       "  %29 : int = prim::Constant[value=1]()\n",
       "  %30 : Float(*) = aten::sub(%33, %yi, %29)\n",
       "  %24 : int = prim::Constant[value=0]()\n",
       "  %25 : float = prim::Constant[value=inf]()\n",
       "  %hi : Float(*) = aten::clamp(%30, %24, %25)\n",
       "  %area_i : Float(*) = aten::mul(%wi, %hi)\n",
       "  %19 : Float(*) = aten::mul(%17, %18)\n",
       "  %16 : Float(*) = aten::mul(%14, %15)\n",
       "  %12 : int = prim::Constant[value=1]()\n",
       "  %13 : Float(*) = aten::add(%19, %16, %12)\n",
       "  %8 : int = prim::Constant[value=1]()\n",
       "  %area_u : Float(*) = aten::sub(%13, %area_i, %8)\n",
       "  %4 : float = prim::Constant[value=1e-05]()\n",
       "  %5 : float = prim::Constant[value=inf]()\n",
       "  %6 : Float(*) = aten::clamp(%area_u, %4, %5)\n",
       "  %2 : Float(*) = aten::div(%area_i, %6)\n",
       "  return (%2);\n",
       "}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratio_iou_scripted.graph_for(x1, y1, w1, h1, x2, y2, w2, h2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that even if things are shown in a fusion group, it can sometimes happen that the fuser decides it cannot create a kernel.\n",
    "You can observe kernel creation by setting the environment variable `PYTORCH_FUSION_DEBUG=1` (works best on the console, the source code is written to the terminal).\n",
    "\n",
    "But we really wanted to get forward and backward, so let's do that.\n",
    "\n",
    "Here is a bit of digression again, but I'll keep it very short: Note that I use `requires_grad_()` below instead of a `requires_grad=True` argument in the `randn`. This is because now `x1` and friends are leaf variables to the autograd graph, otherwise the random tensor (not assigned a Python variable) would be the leaf variables and accumulate the grads! This is something that you can easily fool yourself with (I can't say it never happened to me before and it's a not-so-infrequent cause for people asking on the forums, too). I prefer `.requires_grad_()` over setting the attribute `.requires_grad = True` because the first is not only shorter, but also will fail if I misspell it for any reason.\n",
    "\n",
    "But so here is timing this with backward (I always evaluate the scripted function to not have the one-off compilation time in the timing):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check: 1.1920928955078125e-07 9.5367431640625e-07\n",
      "100 loops, best of 3: 5.3 ms per loop\n",
      "1000 loops, best of 3: 1.17 ms per loop\n"
     ]
    }
   ],
   "source": [
    "x1, y1, w1, h1, x2, y2, w2, h2 = [t.requires_grad_() for t in torch.randn(8, 100_000, device='cuda').exp()]\n",
    "l1 = ratio_iou(x1, y1, w1, h1, x2, y2, w2, h2)\n",
    "l2 = ratio_iou_scripted(x1, y1, w1, h1, x2, y2, w2, h2)\n",
    "grad_out = torch.randn_like(l1)\n",
    "grads1 = torch.autograd.grad(l1, [x1, y1, w1, h1, x2, y2, w2, h2], grad_out)\n",
    "grads2 = torch.autograd.grad(l2, [x1, y1, w1, h1, x2, y2, w2, h2], grad_out)\n",
    "\n",
    "print (\"check:\", (l1-l2).abs().max().item(), max([(g1-g2).abs().max().item() for g1, g2 in zip(grads1, grads2)]))\n",
    "\n",
    "def time_loss_and_backward(fn):\n",
    "    l = fn(x1, y1, w1, h1, x2, y2, w2, h2)\n",
    "    grads = torch.autograd.grad(l, [x1, y1, w1, h1, x2, y2, w2, h2], grad_out)\n",
    "    torch.cuda.synchronize()\n",
    "torch.cuda.synchronize()\n",
    "%timeit time_loss_and_backward(ratio_iou)\n",
    "%timeit time_loss_and_backward(ratio_iou_scripted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I get a 4.5x speedup. Not bad for just adding ` @torch.jit.script`!\n",
    "\n",
    "My measurements have been done on my [PR #14957](https://github.com/pytorch/pytorch/pull/14957) branch. The backward optimization has had a bit of a bumpy ride in PyTorch in November 2018, as a late fix for correct gradients of broadcasted tensors has inserted summations into the backward that cannot be fused. I hope that it will be fixed soon.\n",
    "\n",
    "Let's look at the graph again. You see that it now is wrapped in a `DifferentiableGraph`. This means that the JIT autodiff has identified a block that it knows how to differentiate. Ìnside, you have the `FusionGroup` we already saw and a bit of broadcasting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "graph(%x1 : Float(*)\n",
       "      %y1 : Float(*)\n",
       "      %w1 : Float(*)\n",
       "      %h1 : Float(*)\n",
       "      %x2 : Float(*)\n",
       "      %y2 : Float(*)\n",
       "      %w2 : Float(*)\n",
       "      %h2 : Float(*)) {\n",
       "  %32 : Float(*) = prim::DifferentiableGraph_0(%w2, %h2, %w1, %h1, %y2, %y1, %x2, %x1)\n",
       "  return (%32);\n",
       "}\n",
       "with prim::DifferentiableGraph_0 = graph(%14 : Float(*)\n",
       "      %15 : Float(*)\n",
       "      %17 : Float(*)\n",
       "      %18 : Float(*)\n",
       "      %34 : Float(*)\n",
       "      %37 : Float(*)\n",
       "      %51 : Float(*)\n",
       "      %54 : Float(*)) {\n",
       "  %334 : Float(*), %335 : Float(*), %area_u.1 : Float(*), %area_i.1 : Float(*), %hi.1 : Float(*), %342 : Float(*), %344 : Float(*), %345 : Float(*), %wi.1 : Float(*), %347 : Float(*), %349 : Float(*), %350 : Float(*) = prim::FusionGroup_0(%14, %15, %17, %18, %34, %37, %51, %54)\n",
       "  %353 : int[] = aten::size(%14)\n",
       "  %354 : int[] = aten::size(%15)\n",
       "  %355 : int[] = aten::size(%17)\n",
       "  %356 : int[] = aten::size(%18)\n",
       "  %357 : int[] = aten::size(%34)\n",
       "  %358 : int[] = aten::size(%37)\n",
       "  %359 : int[] = aten::size(%51)\n",
       "  %360 : int[] = aten::size(%54)\n",
       "  %367 : int[] = aten::size(%344)\n",
       "  %368 : int[] = aten::size(%345)\n",
       "  %371 : int[] = aten::size(%349)\n",
       "  %372 : int[] = aten::size(%350)\n",
       "  %373 : int[] = prim::BroadcastSizes(%360, %359)\n",
       "  %374 : int[] = prim::BroadcastSizes(%358, %357)\n",
       "  %377 : int[] = prim::BroadcastSizes(%372, %371)\n",
       "  %381 : int[] = prim::BroadcastSizes(%368, %367)\n",
       "  %384 : int[] = prim::BroadcastSizes(%355, %356)\n",
       "  %385 : int[] = prim::BroadcastSizes(%353, %354)\n",
       "  %386 : int[] = prim::BroadcastSizes(%384, %385)\n",
       "  return (%334, %350, %349, %377, %373, %347, %wi.1, %345, %344, %381, %374, %342, %hi.1, %area_i.1, %384, %385, %386, %area_u.1, %335);\n",
       "}\n",
       "with prim::FusionGroup_0 = graph(%14 : Float(*)\n",
       "      %15 : Float(*)\n",
       "      %17 : Float(*)\n",
       "      %18 : Float(*)\n",
       "      %34 : Float(*)\n",
       "      %37 : Float(*)\n",
       "      %51 : Float(*)\n",
       "      %54 : Float(*)) {\n",
       "  %xi : Float(*) = aten::max(%54, %51)\n",
       "  %yi : Float(*) = aten::max(%37, %34)\n",
       "  %55 : int = prim::Constant[value=1]()\n",
       "  %56 : Float(*) = aten::add(%54, %17, %55)\n",
       "  %52 : int = prim::Constant[value=1]()\n",
       "  %53 : Float(*) = aten::add(%51, %14, %52)\n",
       "  %50 : Float(*) = aten::min(%56, %53)\n",
       "  %46 : int = prim::Constant[value=1]()\n",
       "  %47 : Float(*) = aten::sub(%50, %xi, %46)\n",
       "  %41 : int = prim::Constant[value=0]()\n",
       "  %42 : float = prim::Constant[value=inf]()\n",
       "  %wi.1 : Float(*) = aten::clamp(%47, %41, %42)\n",
       "  %38 : int = prim::Constant[value=1]()\n",
       "  %39 : Float(*) = aten::add(%37, %18, %38)\n",
       "  %35 : int = prim::Constant[value=1]()\n",
       "  %36 : Float(*) = aten::add(%34, %15, %35)\n",
       "  %33 : Float(*) = aten::min(%39, %36)\n",
       "  %29 : int = prim::Constant[value=1]()\n",
       "  %30 : Float(*) = aten::sub(%33, %yi, %29)\n",
       "  %24 : int = prim::Constant[value=0]()\n",
       "  %25 : float = prim::Constant[value=inf]()\n",
       "  %hi.1 : Float(*) = aten::clamp(%30, %24, %25)\n",
       "  %area_i.1 : Float(*) = aten::mul(%wi.1, %hi.1)\n",
       "  %19 : Float(*) = aten::mul(%17, %18)\n",
       "  %16 : Float(*) = aten::mul(%14, %15)\n",
       "  %12 : int = prim::Constant[value=1]()\n",
       "  %13 : Float(*) = aten::add(%19, %16, %12)\n",
       "  %8 : int = prim::Constant[value=1]()\n",
       "  %area_u.1 : Float(*) = aten::sub(%13, %area_i.1, %8)\n",
       "  %4 : float = prim::Constant[value=1e-05]()\n",
       "  %5 : float = prim::Constant[value=inf]()\n",
       "  %6 : Float(*) = aten::clamp(%area_u.1, %4, %5)\n",
       "  %2 : Float(*) = aten::div(%area_i.1, %6)\n",
       "  return (%2, %6, %area_u.1, %area_i.1, %hi.1, %30, %36, %39, %wi.1, %47, %53, %56);\n",
       "}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratio_iou_scripted.graph_for(x1, y1, w1, h1, x2, y2, w2, h2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the backward graph, too. I extracted the code to get the backward graph from PyTorch's testsuite. I re-define the function in order for only a single backward being defined. It tries to extract the backward graph from the latest(?) run forward, so it might be a bit fragile (rerun the definition of the ratio_iou_script and the timing with backward before backward_graph) if you run into trouble.\n",
    "Note that in the output here, the bulk of the calculation (except a few `GradSumToSize`) is done in a large fusion group again. On 1.0 this would have been split into piecemeal fusiongroups with `SumToSize` in between."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_graph(script_module):\n",
    "    # magic debugging stuff I learned about in the PyTorch JIT test suite\n",
    "    graph_executor_state = script_module.get_debug_state()\n",
    "    fwd_plan = list(graph_executor_state.execution_plans.values())[-1]\n",
    "    grad_executor = list(fwd_plan.code.grad_executors())[-1]\n",
    "    bwd_plan = list(grad_executor.get_debug_state().execution_plans.values())[-1]\n",
    "    return bwd_plan.graph.copy() # in order to own the graph, we need to make a copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "graph(%0 : Float(*)\n",
       "      %1 : UndefinedTensor\n",
       "      %2 : UndefinedTensor\n",
       "      %3 : UndefinedTensor\n",
       "      %4 : UndefinedTensor\n",
       "      %5 : UndefinedTensor\n",
       "      %6 : UndefinedTensor\n",
       "      %7 : UndefinedTensor\n",
       "      %8 : UndefinedTensor\n",
       "      %9 : UndefinedTensor\n",
       "      %10 : UndefinedTensor\n",
       "      %11 : UndefinedTensor\n",
       "      %12 : Float(*)\n",
       "      %13 : Float(*)\n",
       "      %14 : Float(*)\n",
       "      %15 : Float(*)\n",
       "      %16 : Float(*)\n",
       "      %17 : Float(*)\n",
       "      %18 : Float(*)\n",
       "      %19 : Float(*)\n",
       "      %20 : Float(*)\n",
       "      %21 : Float(*)\n",
       "      %22 : int[]\n",
       "      %23 : int[]\n",
       "      %24 : Float(*)\n",
       "      %wi : Float(*)\n",
       "      %26 : Float(*)\n",
       "      %27 : Float(*)\n",
       "      %28 : int[]\n",
       "      %29 : int[]\n",
       "      %30 : Float(*)\n",
       "      %hi : Float(*)\n",
       "      %area_i : Float(*)\n",
       "      %33 : int[]\n",
       "      %34 : int[]\n",
       "      %35 : int[]\n",
       "      %area_u : Float(*)\n",
       "      %37 : Float(*)) {\n",
       "  %38 : int[] = aten::size(%13)\n",
       "  %39 : int[] = aten::size(%15)\n",
       "  %40 : int[] = aten::size(%12)\n",
       "  %41 : int[] = aten::size(%14)\n",
       "  %42 : int[] = aten::size(%16)\n",
       "  %43 : int[] = aten::size(%17)\n",
       "  %44 : int[] = aten::size(%18)\n",
       "  %45 : int[] = aten::size(%19)\n",
       "  %46 : Tensor, %47 : Tensor, %48 : Tensor, %49 : Tensor, %50 : Tensor, %51 : Tensor, %52 : Tensor, %53 : Tensor = prim::FusionGroup_0(%15, %14, %12, %wi, %13, %18, %19, %21, %20, %24, %hi, %area_u, %area_i, %0, %37, %27, %26, %30, %16, %17)\n",
       "  %54 : Tensor = prim::AutodiffGradSumToSize(%46, %45)\n",
       "  %55 : Tensor = prim::AutodiffGradSumToSize(%47, %41)\n",
       "  %56 : Tensor = prim::AutodiffGradSumToSize(%48, %39)\n",
       "  %57 : Tensor = prim::AutodiffGradSumToSize(%49, %43)\n",
       "  %58 : Tensor = prim::AutodiffGradSumToSize(%50, %42)\n",
       "  %59 : Tensor = prim::AutodiffGradSumToSize(%51, %38)\n",
       "  %60 : Tensor = prim::AutodiffGradSumToSize(%52, %40)\n",
       "  %61 : Tensor = prim::AutodiffGradSumToSize(%53, %44)\n",
       "  return (%60, %59, %55, %56, %58, %57, %61, %54);\n",
       "}\n",
       "with prim::FusionGroup_0 = graph(%23 : Float(*)\n",
       "      %42 : Float(*)\n",
       "      %103 : Float(*)\n",
       "      %122 : Float(*)\n",
       "      %139 : Float(*)\n",
       "      %161 : Float(*)\n",
       "      %162 : Float(*)\n",
       "      %169 : Float(*)\n",
       "      %170 : Float(*)\n",
       "      %198 : Float(*)\n",
       "      %235 : Float(*)\n",
       "      %271 : Float(*)\n",
       "      %312 : Float(*)\n",
       "      %314 : Float(*)\n",
       "      %316 : Float(*)\n",
       "      %321 : Float(*)\n",
       "      %322 : Float(*)\n",
       "      %336 : Float(*)\n",
       "      %367 : Float(*)\n",
       "      %368 : Float(*)) {\n",
       "  %373 : Byte(*) = aten::gt(%162, %161)\n",
       "  %372 : Byte(*) = aten::lt(%170, %169)\n",
       "  %371 : Byte(*) = aten::lt(%322, %321)\n",
       "  %370 : Byte(*) = aten::gt(%368, %367)\n",
       "  %369 : Byte(*) = aten::gt(%367, %368)\n",
       "  %366 : float = prim::Constant[value=inf]()\n",
       "  %365 : float = prim::Constant[value=inf]()\n",
       "  %364 : float = prim::Constant[value=inf]()\n",
       "  %363 : float = prim::Constant[value=inf]()\n",
       "  %362 : float = prim::Constant[value=inf]()\n",
       "  %361 : float = prim::Constant[value=inf]()\n",
       "  %359 : float = prim::Constant[value=inf]()\n",
       "  %360 : Byte(*) = aten::ge(%336, %359)\n",
       "  %358 : Float(*) = aten::type_as(%360, %336)\n",
       "  %356 : Float(*) = aten::neg(%358)\n",
       "  %354 : int = prim::Constant[value=1]()\n",
       "  %353 : int = prim::Constant[value=1]()\n",
       "  %352 : int = prim::Constant[value=1]()\n",
       "  %351 : int = prim::Constant[value=1]()\n",
       "  %350 : int = prim::Constant[value=1]()\n",
       "  %349 : int = prim::Constant[value=1]()\n",
       "  %347 : int = prim::Constant[value=1]()\n",
       "  %348 : Float(*) = aten::add(%356, %347, %347)\n",
       "  %345 : int = prim::Constant[value=0]()\n",
       "  %344 : int = prim::Constant[value=0]()\n",
       "  %343 : int = prim::Constant[value=0]()\n",
       "  %342 : int = prim::Constant[value=0]()\n",
       "  %341 : int = prim::Constant[value=0]()\n",
       "  %340 : int = prim::Constant[value=0]()\n",
       "  %338 : int = prim::Constant[value=0]()\n",
       "  %339 : Byte(*) = aten::le(%336, %338)\n",
       "  %337 : Float(*) = aten::type_as(%339, %336)\n",
       "  %334 : Float(*) = aten::neg(%337)\n",
       "  %332 : int = prim::Constant[value=1]()\n",
       "  %331 : int = prim::Constant[value=1]()\n",
       "  %330 : int = prim::Constant[value=1]()\n",
       "  %329 : int = prim::Constant[value=1]()\n",
       "  %328 : int = prim::Constant[value=1]()\n",
       "  %327 : int = prim::Constant[value=1]()\n",
       "  %325 : int = prim::Constant[value=1]()\n",
       "  %326 : Float(*) = aten::add(%334, %325, %325)\n",
       "  %323 : Byte(*) = aten::lt(%321, %322)\n",
       "  %320 : Float(*) = aten::div(%314, %316)\n",
       "  %317 : Float(*) = aten::mul(%316, %316)\n",
       "  %315 : Float(*) = aten::neg(%314)\n",
       "  %313 : Float(*) = aten::mul(%315, %312)\n",
       "  %310 : Float(*) = aten::div(%313, %317)\n",
       "  %304 : float = prim::Constant[value=inf]()\n",
       "  %303 : float = prim::Constant[value=inf]()\n",
       "  %302 : float = prim::Constant[value=inf]()\n",
       "  %301 : float = prim::Constant[value=inf]()\n",
       "  %300 : float = prim::Constant[value=inf]()\n",
       "  %299 : float = prim::Constant[value=inf]()\n",
       "  %298 : float = prim::Constant[value=inf]()\n",
       "  %296 : float = prim::Constant[value=inf]()\n",
       "  %297 : Byte(*) = aten::ge(%271, %296)\n",
       "  %295 : Float(*) = aten::type_as(%297, %271)\n",
       "  %293 : Float(*) = aten::neg(%295)\n",
       "  %291 : int = prim::Constant[value=1]()\n",
       "  %290 : int = prim::Constant[value=1]()\n",
       "  %289 : int = prim::Constant[value=1]()\n",
       "  %288 : int = prim::Constant[value=1]()\n",
       "  %287 : int = prim::Constant[value=1]()\n",
       "  %286 : int = prim::Constant[value=1]()\n",
       "  %285 : int = prim::Constant[value=1]()\n",
       "  %283 : int = prim::Constant[value=1]()\n",
       "  %284 : Float(*) = aten::add(%293, %283, %283)\n",
       "  %281 : float = prim::Constant[value=1e-05]()\n",
       "  %280 : float = prim::Constant[value=1e-05]()\n",
       "  %279 : float = prim::Constant[value=1e-05]()\n",
       "  %278 : float = prim::Constant[value=1e-05]()\n",
       "  %277 : float = prim::Constant[value=1e-05]()\n",
       "  %276 : float = prim::Constant[value=1e-05]()\n",
       "  %275 : float = prim::Constant[value=1e-05]()\n",
       "  %273 : float = prim::Constant[value=1e-05]()\n",
       "  %274 : Byte(*) = aten::le(%271, %273)\n",
       "  %272 : Float(*) = aten::type_as(%274, %271)\n",
       "  %269 : Float(*) = aten::neg(%272)\n",
       "  %267 : int = prim::Constant[value=1]()\n",
       "  %266 : int = prim::Constant[value=1]()\n",
       "  %265 : int = prim::Constant[value=1]()\n",
       "  %264 : int = prim::Constant[value=1]()\n",
       "  %263 : int = prim::Constant[value=1]()\n",
       "  %262 : int = prim::Constant[value=1]()\n",
       "  %261 : int = prim::Constant[value=1]()\n",
       "  %259 : int = prim::Constant[value=1]()\n",
       "  %260 : Float(*) = aten::add(%269, %259, %259)\n",
       "  %257 : Tensor = aten::mul(%310, %260)\n",
       "  %254 : Tensor = aten::mul(%257, %284)\n",
       "  %251 : Tensor = aten::neg(%254)\n",
       "  %247 : int = prim::Constant[value=1]()\n",
       "  %246 : int = prim::Constant[value=1]()\n",
       "  %245 : int = prim::Constant[value=1]()\n",
       "  %244 : int = prim::Constant[value=1]()\n",
       "  %243 : int = prim::Constant[value=1]()\n",
       "  %242 : int = prim::Constant[value=1]()\n",
       "  %241 : int = prim::Constant[value=1]()\n",
       "  %239 : int = prim::Constant[value=1]()\n",
       "  %240 : Tensor = aten::add(%320, %251, %239)\n",
       "  %236 : Tensor = aten::mul(%240, %235)\n",
       "  %231 : float = prim::Constant[value=inf]()\n",
       "  %230 : float = prim::Constant[value=inf]()\n",
       "  %229 : float = prim::Constant[value=inf]()\n",
       "  %228 : float = prim::Constant[value=inf]()\n",
       "  %227 : float = prim::Constant[value=inf]()\n",
       "  %226 : float = prim::Constant[value=inf]()\n",
       "  %225 : float = prim::Constant[value=inf]()\n",
       "  %223 : float = prim::Constant[value=inf]()\n",
       "  %224 : Byte(*) = aten::ge(%198, %223)\n",
       "  %222 : Float(*) = aten::type_as(%224, %198)\n",
       "  %220 : Float(*) = aten::neg(%222)\n",
       "  %218 : int = prim::Constant[value=1]()\n",
       "  %217 : int = prim::Constant[value=1]()\n",
       "  %216 : int = prim::Constant[value=1]()\n",
       "  %215 : int = prim::Constant[value=1]()\n",
       "  %214 : int = prim::Constant[value=1]()\n",
       "  %213 : int = prim::Constant[value=1]()\n",
       "  %212 : int = prim::Constant[value=1]()\n",
       "  %210 : int = prim::Constant[value=1]()\n",
       "  %211 : Float(*) = aten::add(%220, %210, %210)\n",
       "  %208 : int = prim::Constant[value=0]()\n",
       "  %207 : int = prim::Constant[value=0]()\n",
       "  %206 : int = prim::Constant[value=0]()\n",
       "  %205 : int = prim::Constant[value=0]()\n",
       "  %204 : int = prim::Constant[value=0]()\n",
       "  %203 : int = prim::Constant[value=0]()\n",
       "  %202 : int = prim::Constant[value=0]()\n",
       "  %200 : int = prim::Constant[value=0]()\n",
       "  %201 : Byte(*) = aten::le(%198, %200)\n",
       "  %199 : Float(*) = aten::type_as(%201, %198)\n",
       "  %196 : Float(*) = aten::neg(%199)\n",
       "  %194 : int = prim::Constant[value=1]()\n",
       "  %193 : int = prim::Constant[value=1]()\n",
       "  %192 : int = prim::Constant[value=1]()\n",
       "  %191 : int = prim::Constant[value=1]()\n",
       "  %190 : int = prim::Constant[value=1]()\n",
       "  %189 : int = prim::Constant[value=1]()\n",
       "  %188 : int = prim::Constant[value=1]()\n",
       "  %186 : int = prim::Constant[value=1]()\n",
       "  %187 : Float(*) = aten::add(%196, %186, %186)\n",
       "  %184 : Tensor = aten::mul(%236, %187)\n",
       "  %181 : Tensor = aten::mul(%184, %211)\n",
       "  %176 : Tensor = aten::neg(%181)\n",
       "  %171 : Byte(*) = aten::lt(%169, %170)\n",
       "  %168 : Tensor = aten::type_as(%171, %181)\n",
       "  %166 : Tensor = aten::mul(%181, %168)\n",
       "  %163 : Byte(*) = aten::gt(%161, %162)\n",
       "  %160 : Tensor = aten::type_as(%163, %176)\n",
       "  %158 : Tensor = aten::mul(%176, %160)\n",
       "  %153 : int = prim::Constant[value=1]()\n",
       "  %152 : int = prim::Constant[value=1]()\n",
       "  %151 : int = prim::Constant[value=1]()\n",
       "  %150 : int = prim::Constant[value=1]()\n",
       "  %149 : int = prim::Constant[value=1]()\n",
       "  %148 : int = prim::Constant[value=1]()\n",
       "  %147 : int = prim::Constant[value=1]()\n",
       "  %145 : int = prim::Constant[value=1]()\n",
       "  %146 : Tensor = aten::add(%166, %158, %145)\n",
       "  %140 : Tensor = aten::mul(%254, %139)\n",
       "  %133 : int = prim::Constant[value=1]()\n",
       "  %132 : int = prim::Constant[value=1]()\n",
       "  %131 : int = prim::Constant[value=1]()\n",
       "  %130 : int = prim::Constant[value=1]()\n",
       "  %129 : int = prim::Constant[value=1]()\n",
       "  %128 : int = prim::Constant[value=1]()\n",
       "  %126 : int = prim::Constant[value=1]()\n",
       "  %127 : Tensor = aten::add(%140, %166, %126)\n",
       "  %123 : Tensor = aten::mul(%240, %122)\n",
       "  %117 : Tensor = aten::mul(%123, %326)\n",
       "  %114 : Tensor = aten::mul(%117, %348)\n",
       "  %109 : Tensor = aten::type_as(%323, %114)\n",
       "  %107 : Tensor = aten::mul(%114, %109)\n",
       "  %104 : Tensor = aten::mul(%254, %103)\n",
       "  %98 : int = prim::Constant[value=1]()\n",
       "  %97 : int = prim::Constant[value=1]()\n",
       "  %96 : int = prim::Constant[value=1]()\n",
       "  %95 : int = prim::Constant[value=1]()\n",
       "  %94 : int = prim::Constant[value=1]()\n",
       "  %92 : int = prim::Constant[value=1]()\n",
       "  %93 : Tensor = aten::add(%104, %107, %92)\n",
       "  %89 : Tensor = aten::neg(%114)\n",
       "  %84 : Tensor = aten::type_as(%369, %89)\n",
       "  %82 : Tensor = aten::mul(%89, %84)\n",
       "  %75 : int = prim::Constant[value=1]()\n",
       "  %74 : int = prim::Constant[value=1]()\n",
       "  %73 : int = prim::Constant[value=1]()\n",
       "  %72 : int = prim::Constant[value=1]()\n",
       "  %70 : int = prim::Constant[value=1]()\n",
       "  %71 : Tensor = aten::add(%107, %82, %70)\n",
       "  %67 : Tensor = aten::type_as(%371, %114)\n",
       "  %65 : Tensor = aten::mul(%114, %67)\n",
       "  %61 : Tensor = aten::type_as(%370, %89)\n",
       "  %59 : Tensor = aten::mul(%89, %61)\n",
       "  %53 : int = prim::Constant[value=1]()\n",
       "  %52 : int = prim::Constant[value=1]()\n",
       "  %51 : int = prim::Constant[value=1]()\n",
       "  %49 : int = prim::Constant[value=1]()\n",
       "  %50 : Tensor = aten::add(%65, %59, %49)\n",
       "  %43 : Tensor = aten::mul(%254, %42)\n",
       "  %36 : int = prim::Constant[value=1]()\n",
       "  %35 : int = prim::Constant[value=1]()\n",
       "  %33 : int = prim::Constant[value=1]()\n",
       "  %34 : Tensor = aten::add(%43, %65, %33)\n",
       "  %30 : Tensor = aten::type_as(%372, %181)\n",
       "  %28 : Tensor = aten::mul(%181, %30)\n",
       "  %24 : Tensor = aten::mul(%254, %23)\n",
       "  %18 : int = prim::Constant[value=1]()\n",
       "  %16 : int = prim::Constant[value=1]()\n",
       "  %17 : Tensor = aten::add(%24, %28, %16)\n",
       "  %13 : Tensor = aten::type_as(%373, %176)\n",
       "  %11 : Tensor = aten::mul(%176, %13)\n",
       "  %2 : int = prim::Constant[value=1]()\n",
       "  %3 : Tensor = aten::add(%28, %11, %2)\n",
       "  return (%3, %17, %34, %50, %71, %93, %127, %146);\n",
       "}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "backward_graph(ratio_iou_scripted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's all for now. I hope you enjoyed this little demo. I hope you enjoyed it and appreciate your feedback and comments at <tv@lernapparat.de>.\n",
    "\n",
    "On my blog https://lernapparat.de/ you'll find the slides from the talk that this demonstration accompanies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
