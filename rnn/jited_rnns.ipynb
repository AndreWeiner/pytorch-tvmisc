{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# JITed RNNs for PyTorch\n",
    "\n",
    "*Thomas Viehmann <tv@lernapparat.de>*\n",
    "\n",
    "Here I'm trying to explore a flexible JITed LSTM / RNN implementation with the hope to eventually merge back an improved LSTM into PyTorch.\n",
    "\n",
    "As usual, it should follow the functional + modular interface convention of PyTorch.\n",
    "\n",
    "\n",
    "What others do as inspiration for the Features/API:\n",
    "- Sonnet [Gated RNN source](https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/gated_rnn.py)\n",
    "- Keras [LSTM docs](https://keras.io/layers/recurrent/#lstm)\n",
    "- FastAI [LSTM](https://github.com/fastai/fastai/blob/master/fastai/text/models/awd_lstm.py)\n",
    "\n",
    "Top desired features (all = LSTM / GRU / RNN:)\n",
    "- Custom activation functions (all)\n",
    "- Layer norm (all)\n",
    "- Custom forget gate bias (LSTM)\n",
    "- Peephole connections (LSTM)\n",
    "- Cell clipping (LSTM)\n",
    "- Projection layer (LSTM) (lowest priority, but seems useful for large-scale applications) (edited) \n",
    "\n",
    "Larger wishlist with links / details: [PyTorch issue #9572](https://github.com/pytorch/pytorch/issues/9572).\n",
    "\n",
    "### Thank you\n",
    "- Kai Arulkumaran helped a lot collating the wishlist, links top desired features\n",
    "\n",
    "Errors and such are all my own.\n",
    "\n",
    "### Help wanted!\n",
    "\n",
    "If you have something to contribute, I'd be most happy. Note that we'll want to balance providing functionality to with providing a concise core PyTorch library.\n",
    "The good news is that you can copypaste the code here to get completely custom JITed fast RNNs!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functional Interfacefor a Cell\n",
    "\n",
    "We start with a functional interface for a cell.\n",
    "\n",
    "One wishlist item is to have custom (recurrent(?)) activations. We define this as a pointwise function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.jit.script\n",
    "def activation_cell(cx):\n",
    "    return torch.tanh(cx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note takes non-parameter jit.script functions activation_cell from context at definition time! (Probably will want to do this in a factory style function even if it's not 100% Pythonic)\n",
    "@torch.jit.script\n",
    "def lstm_cell(input, hidden, w_ih, w_hh, b_ih, b_hh):\n",
    "    # type: (Tensor, Tuple[Tensor, Tensor], Tensor, Tensor, Tensor, Tensor) -> Tuple[Tensor, Tensor]\n",
    "    hx, cx = hidden\n",
    "    gates = torch.mm(input, w_ih.t()) + torch.mm(hx, w_hh.t()) + b_ih + b_hh\n",
    "\n",
    "    ingate, forgetgate, cellgate, outgate = gates.chunk(4, 1)\n",
    "\n",
    "    ingate = torch.sigmoid(ingate)\n",
    "    forgetgate = torch.sigmoid(forgetgate)\n",
    "    cellgate = activation_cell(cellgate)\n",
    "    outgate = torch.sigmoid(outgate)\n",
    "\n",
    "    cy = (forgetgate * cx) + (ingate * cellgate)\n",
    "    hy = outgate * torch.tanh(cy)\n",
    "\n",
    "    return hy, cy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def graph(self,\n",
      "    input: Tensor,\n",
      "    hidden: Tuple[Tensor, Tensor],\n",
      "    w_ih: Tensor,\n",
      "    w_hh: Tensor,\n",
      "    b_ih: Tensor,\n",
      "    b_hh: Tensor) -> Tuple[Tensor, Tensor]:\n",
      "  hx, cx, = hidden\n",
      "  _0 = torch.add(torch.mm(input, torch.t(w_ih)), torch.mm(hx, torch.t(w_hh)), alpha=1)\n",
      "  gates = torch.add(torch.add(_0, b_ih, alpha=1), b_hh, alpha=1)\n",
      "  ingate, forgetgate, cellgate, outgate, = torch.chunk(gates, 4, 1)\n",
      "  ingate0 = torch.sigmoid(ingate)\n",
      "  forgetgate0 = torch.sigmoid(forgetgate)\n",
      "  cellgate0 = torch.tanh(cellgate)\n",
      "  outgate0 = torch.sigmoid(outgate)\n",
      "  cy = torch.add(torch.mul(forgetgate0, cx), torch.mul(ingate0, cellgate0), alpha=1)\n",
      "  hy = torch.mul(outgate0, torch.tanh(cy))\n",
      "  return (hy, cy)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(lstm_cell.graph.pretty_print())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
